{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hva2Vb8hBKoJ",
    "outputId": "db3a70eb-e97e-4e4f-8386-0a1293bbf37f"
   },
   "outputs": [],
   "source": [
    "pip install pandas nltk emoji scikit-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o0lf7Hoj7wy0",
    "outputId": "0e9aad83-b807-444e-daa7-2fe1a0c50af6"
   },
   "outputs": [],
   "source": [
    "# !wget http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\n",
    "# !unzip trainingandtestdata.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-wTpDQG1BL9z",
    "outputId": "8d6b6fa1-df30-4186-f4c8-fe47cc8adc53"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "file_path = \"/Users/chaitanyam/Downloads/trainingandtestdata/training.1600000.processed.noemoticon.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path, encoding=\"ISO-8859-1\", header=None)\n",
    "df.columns = ['target', 'id', 'date', 'flag', 'user', 'text']\n",
    "df['label'] = df['target'].map({0: 'negative', 4: 'positive'})\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text)\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)  # HTML tags\n",
    "    text = re.sub(r\"@\\w+|#\\w+\", \"\", text)  # mentions and hashtags\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)  # emojis\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s.,!?;:]\", \"\", text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.lower().strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "df['clean_text'] = df['text'].apply(clean_text)\n",
    "df = df.drop_duplicates(subset=['clean_text'])\n",
    "df = df[df['clean_text'].str.strip() != '']\n",
    "df = df.dropna(subset=['clean_text'])\n",
    "\n",
    "print(f\"\\nData cleaned: {len(df):,} samples\\n\")\n",
    "\n",
    "print(\"Some of the Cleaned Tweets:\\n\")\n",
    "print(df[['text', 'clean_text', 'label']].sample(4, random_state=42).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 756
    },
    "id": "h5o1lts7BR-3",
    "outputId": "bfd865e1-849b-447d-960c-c0450fefd18b"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "\n",
    "df['word_tokens'] = df['clean_text'].apply(word_tokenize)\n",
    "df['word_count'] = df['word_tokens'].apply(len)\n",
    "\n",
    "\n",
    "df['sentence_tokens'] = df['clean_text'].apply(sent_tokenize)\n",
    "df['sentence_count'] = df['sentence_tokens'].apply(len)\n",
    "\n",
    "\n",
    "df[['clean_text', 'word_tokens', 'word_count', 'sentence_tokens', 'sentence_count']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return \" \".join([lemmatizer.lemmatize(token) for token in tokens])\n",
    "\n",
    "df['lemmatized_text'] = df['clean_text'].apply(lemmatize)\n",
    "\n",
    "print(\"Sample Lemmatized Tweets:\\n\")\n",
    "print(df[['clean_text', 'lemmatized_text', 'label']].sample(4, random_state=42).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X7J1l4HvBUmq",
    "outputId": "7a7399ec-d673-4b1c-dc7e-01992cc92b5e"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "df['text_length'] = df['clean_text'].str.len()\n",
    "all_words = ' '.join(df['clean_text']).split()\n",
    "vocab_size = len(set(all_words))\n",
    "total_words = len(all_words)\n",
    "\n",
    "print(\"DATASET STATISTICS\")\n",
    "print(f\"Total samples: {len(df):,}\")\n",
    "print(f\"Unique texts: {df['clean_text'].nunique():,}\")\n",
    "\n",
    "label_counts = df['label'].value_counts()\n",
    "for label, count in label_counts.items():\n",
    "    percentage = count / len(df) * 100\n",
    "    print(f\"{label.capitalize()}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"Average text length: {df['text_length'].mean():.1f} characters\")\n",
    "print(f\"Average word count: {df['word_count'].mean():.1f}\")\n",
    "print(f\"Vocabulary size: {vocab_size:,} words\")\n",
    "\n",
    "word_freq = Counter(all_words)\n",
    "print(\"Most common words:\")\n",
    "for i, (word, freq) in enumerate(word_freq.most_common(5), 1):\n",
    "    print(f\"{i}. '{word}': {freq:,}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['clean_text']\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "\n",
    "print(f\"Data splits created:\")\n",
    "print(f\"Training: {len(X_train):,} ({len(X_train)/len(df)*100:.1f}%)\")\n",
    "print(f\"Validation: {len(X_val):,} ({len(X_val)/len(df)*100:.1f}%)\")\n",
    "print(f\"Test: {len(X_test):,} ({len(X_test)/len(df)*100:.1f}%)\")\n",
    "\n",
    "for split_name, split_labels in [('Training', y_train), ('Validation', y_val), ('Test', y_test)]:\n",
    "    neg_pct = (split_labels == 'negative').sum() / len(split_labels) * 100\n",
    "    pos_pct = (split_labels == 'positive').sum() / len(split_labels) * 100\n",
    "    print(f\"{split_name}: {neg_pct:.1f}% negative, {pos_pct:.1f}% positive\")\n",
    "\n",
    "train_df = pd.DataFrame({'text': X_train.values, 'label': y_train.values})\n",
    "val_df = pd.DataFrame({'text': X_val.values, 'label': y_val.values})\n",
    "test_df = pd.DataFrame({'text': X_test.values, 'label': y_test.values})\n",
    "\n",
    "train_df.to_csv(\"train.csv\", index=False)\n",
    "val_df.to_csv(\"val.csv\", index=False)\n",
    "test_df.to_csv(\"test.csv\", index=False)\n",
    "\n",
    "print(f\"Files saved:\")\n",
    "print(f\"train.csv: {len(train_df):,} samples\")\n",
    "print(f\"val.csv: {len(val_df):,} samples\")\n",
    "print(f\"test.csv: {len(test_df):,} samples\")\n",
    "print(\"Processing completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Label counts and percentages\n",
    "label_counts = df['label'].value_counts()\n",
    "label_percent = df['label'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"Sentiment Label Distribution:\\n\")\n",
    "for label in label_counts.index:\n",
    "    print(f\"{label.capitalize()}: {label_counts[label]:,} tweets ({label_percent[label]:.2f}%)\")\n",
    "\n",
    "print(\"Average Tweet Length (in characters) per Sentiment:\\n\")\n",
    "for label in df['label'].unique():\n",
    "    avg_length = df[df['label'] == label]['clean_text'].str.len().mean()\n",
    "    print(f\"{label.capitalize()}: {avg_length:.1f} characters\")\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x='label', data=df, hue='label', palette='Set2', dodge=False, legend=False)\n",
    "plt.title(\"Label Distribution\")\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
